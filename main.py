import os
import IOB
import subprocess
from huggingface_hub import snapshot_download
from typing import List, Tuple
import labelPropagation
from convert import convert_to_json


def printPercentages(sentences: List[List[Tuple[str, str]]]) -> None:
    """
    Print the percentage of tokens with and without labels in the given sentences.

    Args:
        - sentences (List[List[Tuple[str, str]]]): A list of sentences, where each sentence is a list of tuples.

    Returns:
        - None
    """
    withLabel = 0
    withoutLabel = 0

    for _, sentence in enumerate(sentences):
        for _, token in enumerate(sentence):
            if len(token) > 1:
                withLabel += 1
            else:
                withoutLabel += 1

    # Print percentages
    print(
        "Percentage with label: {:.2f}%".format(
            withLabel / (withLabel + withoutLabel) * 100
        )
    )
    print(
        "Percentage without label: {:.2f}%".format(
            withoutLabel / (withLabel + withoutLabel) * 100
        )
    )


def divideSentences(
    sentences: List[List[Tuple[str, str]]],
) -> Tuple[List[List[Tuple[str, str]]], List[List[Tuple[str, str]]]]:
    """
    Divide the sentences into two parts based on the given percentage.

    Args:
        - sentences (List[List[Tuple[str, str]]]): A list of sentences, where each sentence is a list of tuples.

    Returns:
        - Tuple[List[List[Tuple[str, str]]], List[List[Tuple[str, str]]]]: Two lists of sentences.
    """
    index = 0

    # Calculate the index to split the sentences
    for _, sentence in enumerate(sentences):
        for _, token in enumerate(sentence):
            if len(token) > 1:
                index += 1
            else:
                break

    # Split the sentences into two parts
    sentences1 = []
    sentences2 = []
    for _, sentence in enumerate(sentences):
        if index > 0:
            sentences1.append(sentence)
            index -= len(sentence)
        else:
            sentences2.append(sentence)
    return sentences1, sentences2


def saveFile(sentences: List[List[Tuple[str, str]]], filename: str) -> None:
    """
    Save the sentences to a file.

    Args:
        - sentences (List[List[Tuple[str, str]]]): A list of sentences, where each sentence is a list of tuples.
        - filename (str): The name of the file to save the sentences.

    Returns:
        - None
    """
    with open(filename, "w", encoding="utf-8") as f:
        for sentence in sentences:
            for token in sentence:
                f.write(" ".join(token) + "\n")
            f.write("\n")


def process_file(file_path: str, output_path_prefix: str) -> None:
    """
    Processes a file containing sentences with labeled and unlabeled tokens, applies label propagation if needed,
    and saves the results to output files.

    Args:
        - file_path (str): Path to the input file to be processed.
        - output_path_prefix (str): Prefix for the output files generated by the function.

    Returns:
        - None
    """

    iob = IOB.IOB()
    sentences = iob.parse_file(file_path)

    print(f"\n--- Procesando {file_path} ---\n")
    printPercentages(sentences)

    withLabels, unlabeled = divideSentences(sentences)
    saveFile(unlabeled, f"{output_path_prefix}_unlabeled.csv")

    if not unlabeled:
        print("Todos los tokens ya están etiquetados. No se aplica propagación.")
        saveFile(withLabels, f"{output_path_prefix}.csv")
        return

    labelPropagation.labelingOptions(sentences, len(withLabels))
    consensus = labelPropagation.getConsensus()

    saveFile(withLabels + consensus, f"{output_path_prefix}.csv")
    labelPropagation.checkWithRegex(consensus, withLabels)


def train(reTrain: bool = False) -> None:
    currentDir = os.path.dirname(os.path.abspath(__file__))

    if reTrain:
        # Start the process
        process = subprocess.Popen(
            [
                os.path.join(currentDir, "run_train.sh"),
                "PlanTL-GOB-ES/roberta-base-bne",
            ],
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            bufsize=1,
        )

        # Print each line as it comes
        for line in process.stdout:
            print(line, end="")

        # Wait for the process to complete
        process.wait()

    else:
        snapshot_download(
            repo_id="daor95/APAU_II-3",
            local_dir=os.path.join(currentDir, "models", "roberta-base-bne-ner"),
            repo_type="model",
        )


if __name__ == "__main__":
    currentDir = os.path.dirname(os.path.abspath(__file__))

    train_csv = os.path.join(currentDir, "data", "ner-es.train.csv")
    valid_csv = os.path.join(currentDir, "data", "ner-es.valid.csv")
    train_json = os.path.join(currentDir, "data", "ner-es.train.json")
    valid_json = os.path.join(currentDir, "data", "ner-es.valid.json")

    # Procesar archivos CSV (anotar etiquetas)
    process_file(
        os.path.join(currentDir, "data", "ner-es.trainOld.csv"),
        train_csv[:-4],  # quita .csv para usar como prefijo
    )

    # Convertir a JSONL
    convert_to_json(train_csv, train_json)

    process_file(
        os.path.join(currentDir, "data", "ner-es.validOld.csv"), valid_csv[:-4]
    )

    # Convertir a JSONL
    convert_to_json(valid_csv, valid_json)

    # Download or train the model
    train()
